# training options
# KALDI_ROOT=/share/nas165/pennylee/kaldi_asr/kaldi python eend/train.py -c examples/train_2spk_minilibri.yaml
attractor_loss_ratio: 1.0
attractor_encoder_dropout: 0.1
attractor_decoder_dropout: 0.1
add_encoder_mask: True
context_size: 7
detach_attractor_loss: False
dev_batchsize: 128
encoder_units: 2048
feature_dim: 23
frame_shift: 80
frame_size: 200
use_last_samples: True
gpus: 0 #0
gradclip: 5
hidden_size: 256
input_transform: logmel_meannorm
log_report_batches_num: 1000
max_epochs: 200 #100
use_former: Transformer # Transformer
model_type: TransformerEDA
noam_warmup_steps: 10000
num_frames: 500
num_speakers: 2
num_workers: 4
optimizer: noam
output_path: result/baseline_redo/models
sampling_rate: 8000
seed: 3 #3
subsampling: 10
time_shuffle: False
train_batchsize: 64 # 64
transformer_encoder_dropout: 0.1
transformer_encoder_n_heads: 4
transformer_encoder_n_layers: 4
train_data_dir: /share/nas165/pennylee/espnet/egs2/mini_librispeech/diar1/data/simu/data/train_clean_5_ns2_beta2_500
valid_data_dir: /share/nas165/pennylee/espnet/egs2/mini_librispeech/diar1/data/simu/data/dev_clean_2_ns2_beta2_500
kr_vad_loss_weight: 0
kr_vad_loss_begin_epoch: 100